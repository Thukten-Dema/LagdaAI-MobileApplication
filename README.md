---

### 📘 AI Powered Sign Language to Voice Assistant for Mute

This mobile application bridges the communication gap for non-verbal individuals by converting sign language gestures into both text and natural-sounding speech in real time. Developed using deep learning, computer vision, and mobile UI/UX principles, the app includes gesture recognition, emergency alert notifications, and a speech synthesis system for inclusivity and accessibility.

#### 🔍 Features
- Real-time gesture recognition using deep learning models
- Voice and text-based output for detected gestures
- Emergency gesture detection with caregiver alert notifications
- User-friendly mobile interface designed with Flutter and Figma
- Backend integration using Firebase for secure data handling

#### ⚙️ Technologies Used
- **Frontend:** Dart, Flutter, Figma, Adobe Illustrator  
- **Backend:** Dart, Firebase  
- **AI/ML Frameworks:** TensorFlow, Keras, PyTorch, Scikit-learn, OpenCV, fast.ai, imgaug  
- **Development Tools:** Visual Studio Code, Google Colab, GitLab (version control)  

#### 🎯 Learning Outcomes
- Gained hands-on experience in training and integrating multimodal neural networks
- Learned agile development practices (SCRUM) for collaborative software engineering
- Applied AI concepts in real-world assistive technology
- Developed proficiency in tools like TensorFlow, Flutter, Firebase, and OpenCV
- Strengthened problem-solving, project management, and teamwork skills
